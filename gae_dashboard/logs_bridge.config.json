// Add a new entry for your metric:
//   metricName: what the metric will be called in stackdriver.  (But
//       see also 'labels', below.)
//
//   query: the bigquery snippet that should match for the metric you
//       are collecting.  It can access the field-names listed in
//       logs_bridge.py:_QUERY_FIELDS.
//       Example queries:
//           SUM(status = 404)
//           AVG(INTEGER(REGEXP_EXTRACT(log_messages,
//                                      r"stats.time.server.wall_ms:(\d+)")))
//
//   labels (optional): a list of labels that are used to disaggregate
//        data.  For an introduction to metric-lables, see
//        https://cloud.google.com/monitoring/api/v3/metrics#intro-time-series
//        Basically, each metricName (above) will resolve to a graph
//        in stackdriver, and each possible label-value will resolve
//        to a line on that graph.  So if labels were ['lang', 'os']
//        you'd have lines for 'en'+'chrome', 'es'+'safari', etc.
//        The list of allowable labels is in logs_bridge.py:_LABELS.
//
//   labelValues (optional): a list of values for that label that we
//        send to stackdriver.  If omitted, all values are used.
//        For instance, if `labels` was `['browser']`, labelValues
//        could be `['IE', 'Chrome', 'Chrome mobile', 'Firefox']`.
//        If multiple lables are used, `labelValues` should be a list
//        of pairs: `[('IE', 'iPhone'), ('Safari', 'Desktop')]` for
//        labels=`['browser', 'device']`.  There are also two
//        special strings you can specify in lieu of a list:
//        * "TOP_VALUES(x)": take the x label-values whose metric-value
//          is the highest for this time interval.  So if we were
//          testing average latency by route, it would be the x
//          routes with the highest average latency in the last
//          time-interval.
//        * "MOST_POPULAR(x)": take the x label-values which garnered
//          the most requests.  So if we were tseting average latency
//          by route, it would be the x routes that received the most
//          traffic in the last time-interval.
//
//   frequency (optional): how often to run this query.  Default is
//        'minutely'.  Other options are 'hourly', 'daily', 'weekly'.
//
//   normalizeByRequests (optional): if present and true, normalize this
//        metric by the number of requests seen in the same time period.
//        That is, we store "<result of query>/<num requests seen>".
//        If the number of requests is 0, we do not store a data point.
//        Default is false.
//
//   normalizeByDaysAgo (optional): if present it must be a number X,
//        normalize this metric by the value of the metric exactly X
//        days ago.  (It's recommended that X be a multiple of 7.)
//        That is, we store "<result of query now>/<result of query a
//        X days ago>". If the value X days ago was 0, we do not store a
//        data point.  Default is 0, meaning not to normalize.
//
//   normalizeByLastDeploy (optional): if present and true, normalize this
//        metric by the value of the metric at the same time-delta from
//        the previous deploy.  That is, if we are currently running a
//        deploy and set-default was called 124 seconds ago, then we will
//        calculate the value of this metric now, and also 124 seconds after
//        the prior set-default (before this one), and store the ratio of
//        the two.  If the value for the previous deploy was 0, we do not
//        store a data point.  Default is false.
[
    {
        "metricName": "logs.status.400.week_over_week",
        "labels": ["module_id"],
        "query": "SUM(status = 400)",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.status.401_or_403.week_over_week",
        "labels": ["module_id"],
        "query": "SUM(status = 401 or status = 403)",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.status.404.week_over_week",
        "labels": ["module_id"],
        "query": "SUM(status = 404)",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.status.500.week_over_week",
        "labels": ["module_id"],
        "query": "SUM(status = 500)",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.status.502_or_503.week_over_week",
        "labels": ["module_id"],
        "query": "SUM(status = 502 or status = 503)",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.distinct_ips_by_browser.week_over_week",
        "labels": ["browser"],
        "labelValues": "TOP_VALUES(9)",
        "query": "COUNT(DISTINCT ip)",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.distinct_ips_by_device.week_over_week",
        "labels": ["device"],
        "query": "COUNT(DISTINCT ip)",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.distinct_ips_by_ka_app.week_over_week",
        "labels": ["KA_APP"],
        "query": "COUNT(DISTINCT ip)",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.distinct_ips_by_os.week_over_week",
        "labels": ["os"],
        "labelValues": "TOP_VALUES(10)",
        "query": "COUNT(DISTINCT ip)",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.distinct_ips_by_language.week_over_week",
        "labels": ["lang"],
        "query": "COUNT(DISTINCT ip)",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true,
        "labelValues": "MOST_POPULAR(8)"
    },
    {
        "metricName": "logs.conversions.problem_attempt.week_over_week",
        "query": "SUM(log_messages CONTAINS '\"conversion\": \"problem_attempt\"')",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.conversions.pageview.week_over_week",
        "query": "SUM(log_messages CONTAINS '\"conversion\": \"pageview\"')",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.conversions.video_started.week_over_week",
        "query": "SUM(log_messages CONTAINS '\"conversion\": \"video_started\"')",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.conversions.article_view.week_over_week",
        "query": "SUM(log_messages CONTAINS '\"conversion\": \"article_view\"')",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.conversions.scratchpad_updated.week_over_week",
        "query": "SUM(log_messages CONTAINS '\"conversion\": \"scratchpad_updated\"')",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.conversions.login.week_over_week",
        "query": "SUM(log_messages CONTAINS '\"conversion\": \"login\"')",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.line_count.week_over_week",
        "query": "COUNT(*)",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": false
    },
    {
        "metricName": "logs.byte_count.week_over_week",
        "query": "SUM(LENGTH(log_messages))",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": false
    },
    {
        "metricName": "logs.latency.week_over_week",
        "labels": ["route"],
        "labelValues": "MOST_POPULAR(20)",
        "query": "AVG(latency)",
        "normalizeByDaysAgo": 7,
        "normalizeByRequests": true
    },
    {
        "metricName": "logs.cronjobs.failures",
        // TODO(csilvers): better to use 'url'
        "labels": ["route"],
        // We are interested in cronjobs where all the jobs run in the
        // last X minutes were failures.  This captures both: frequent
        // (minutely) cronjobs that are failing a lot, and infrequent
        // (daily) cronjobs that fail even once.  The right alert for
        // this is when the value is >= 1.0.
        "query": "SUM(task_queue_name == '__cron' and status >= 300) / SUM(task_queue_name == '__cron')"
    }
]
